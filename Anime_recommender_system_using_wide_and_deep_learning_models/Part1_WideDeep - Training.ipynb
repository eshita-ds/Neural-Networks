{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94f29a3-9eb2-4aad-a070-19b35064e271",
   "metadata": {},
   "source": [
    "## <font color='purple'> Data 255 - Lab 1 - Part 1 Contd..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04bbb3-859b-40be-abb6-6a2043fd00c7",
   "metadata": {
    "id": "ef04bbb3-859b-40be-abb6-6a2043fd00c7"
   },
   "source": [
    "<font color='purple'> Part 1: Deep Learning-Based Recommendation (10 Points)\n",
    "\n",
    "<font color='purple'> Read the paper Wide and Deep Learning for Recommender Systems.\n",
    "\n",
    "<font color='purple'> Download the files anime-dataset-2023.csv, users-details-2023.csv, users-score- 2023.csv from the following link: https://www.kaggle.com/datasets/dbdmobile/myanimelist-dataset\n",
    "\n",
    "<font color='purple'> Based on the architecture described in the paper, build your own Wide and Deep Recommender system for the Anime Dataset. Your model should learn the features of each user and anime, not just the associated ID numbers. Utilize an 80/20 train-test split and record your modelâ€™s prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7b939-b089-4fe1-b7f2-e8d612bf1584",
   "metadata": {
    "id": "88f7b939-b089-4fe1-b7f2-e8d612bf1584"
   },
   "source": [
    "### <font color='blue'> Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897ccd6c-11dc-441b-99a0-62995097d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867a16d-9bd4-43fb-b693-bd51fef0bd1c",
   "metadata": {},
   "source": [
    "<font color='blue'> Load the saved Data files for final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0d5b91-ec7a-4d82-ae7d-82e76009f65f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sampled = pd.read_pickle('data_sampled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cf982b-a1f2-488b-ba6e-6eac127bfa02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11901624, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd1515c-d9d4-4ee7-9748-5771a39c4f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'anime_id', 'Anime Title', 'rating', 'Username', 'Gender',\n",
       "       'Days Watched', 'Mean Score', 'Completed', 'Total Entries', 'Name',\n",
       "       'Score', 'Genres', 'Type', 'Episodes', 'Status', 'Producers', 'Studios',\n",
       "       'Source', 'Rating', 'Rank', 'Popularity', 'Favorites', 'Air Start',\n",
       "       'Air End', 'Duration (min)', 'Type_Gender'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877dd035-eba4-45de-a705-83f1633156e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id              int64\n",
       "anime_id             int64\n",
       "Anime Title         object\n",
       "rating               int64\n",
       "Username            object\n",
       "Gender              object\n",
       "Days Watched       float64\n",
       "Mean Score         float64\n",
       "Completed          float64\n",
       "Total Entries      float64\n",
       "Name                object\n",
       "Score              float64\n",
       "Genres              object\n",
       "Type                object\n",
       "Episodes           float64\n",
       "Status              object\n",
       "Producers           object\n",
       "Studios             object\n",
       "Source              object\n",
       "Rating              object\n",
       "Rank               float64\n",
       "Popularity           int64\n",
       "Favorites            int64\n",
       "Air Start          float64\n",
       "Air End            float64\n",
       "Duration (min)     float64\n",
       "Type_Gender       category\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daefbb6-23d4-4231-a268-10ec410ad152",
   "metadata": {},
   "source": [
    "<font color='blue'> Drop the column used for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a465ed09-60a9-4c39-bc82-58bca90fa1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sampled.drop(columns = ['Type_Gender'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226fbf76-23a1-40fd-9466-5db4486c85c8",
   "metadata": {},
   "source": [
    "<font color='blue'> Setup the Wide and Deep features for training \\\n",
    "    \\\n",
    "    Wide Features - Categorical Features \\\n",
    "    Deep Features - Continous Features \\\n",
    "    \\\n",
    "    Also, perform encoding for wide features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b54b347-4b62-463e-baca-f2d3e46a17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(df):\n",
    "    wide_features = ['user_id', 'anime_id', 'Gender', 'Type', 'Status', 'Producers', \n",
    "                     'Studios', 'Source', 'Rating']\n",
    "    \n",
    "    deep_features = ['Days Watched', 'Mean Score', 'Completed', 'Total Entries', 'Genres', \n",
    "                     'Episodes', 'Rank', 'Popularity', 'Favorites', 'Air Start', 'Air End', 'Duration (min)']\n",
    "    \n",
    "    encoders = {}\n",
    "    for feature in wide_features:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{feature}_encoded'] = le.fit_transform(df[feature].fillna('Unknown'))\n",
    "        encoders[feature] = le\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[deep_features] = df[deep_features].fillna(df[deep_features].mean())\n",
    "    scaled_features = scaler.fit_transform(df[deep_features])\n",
    "    for i, feature in enumerate(deep_features):\n",
    "        df[f'{feature}_scaled'] = scaled_features[:, i]\n",
    "    \n",
    "    return df, wide_features, deep_features, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946e023-aeb1-40d1-8c33-11b120d60e99",
   "metadata": {},
   "source": [
    "<font color='blue'> Setup Dataset Class to be used for preparing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae46689b-e4bc-44ca-927b-75c1f0e54e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, df, wide_features, deep_features):\n",
    "        self.wide_features = torch.tensor(df[[f'{f}_encoded' for f in wide_features]].values)\n",
    "        self.deep_features = torch.tensor(df[[f'{f}_scaled' for f in deep_features]].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df['Score'].values, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'wide': self.wide_features[idx],\n",
    "            'deep': self.deep_features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975a7c0-5944-4d44-9bc0-e58762e8756e",
   "metadata": {},
   "source": [
    "<font color='blue'> Setup the Model Architecture as a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ac67cc-0f49-475c-a5cb-42e083aec8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(nn.Module):\n",
    "    def __init__(self, wide_dim, deep_dim, wide_embeddings_dim, deep_hidden_units):\n",
    "        super(WideAndDeepModel, self).__init__()\n",
    "        \n",
    "        self.wide_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(size, wide_embeddings_dim) \n",
    "            for size in wide_dim\n",
    "        ])\n",
    "        \n",
    "        wide_output_dim = len(wide_dim) * wide_embeddings_dim\n",
    "        \n",
    "        deep_layers = []\n",
    "        input_dim = deep_dim\n",
    "        \n",
    "        for units in deep_hidden_units:\n",
    "            deep_layers.append(nn.Linear(input_dim, units))\n",
    "            deep_layers.append(nn.ReLU())\n",
    "            deep_layers.append(nn.Dropout(0.2))\n",
    "            input_dim = units\n",
    "        \n",
    "        self.deep_layers = nn.Sequential(*deep_layers)\n",
    "        \n",
    "        combined_dim = wide_output_dim + deep_hidden_units[-1]\n",
    "        self.final_layer = nn.Linear(combined_dim, 1)\n",
    "        \n",
    "    def forward(self, wide_input, deep_input):\n",
    "        wide_embeddings = [emb(wide_input[:, i]) for i, emb in enumerate(self.wide_embeddings)]\n",
    "        wide_concat = torch.cat(wide_embeddings, dim=1)\n",
    "        \n",
    "        deep_output = self.deep_layers(deep_input)\n",
    "        \n",
    "        combined = torch.cat([wide_concat, deep_output], dim=1)\n",
    "        prediction = self.final_layer(combined)\n",
    "        \n",
    "        return prediction.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c294bd-c337-4403-8282-02ce187a672d",
   "metadata": {},
   "source": [
    "<font color='blue'> Setup the Train Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a0a4e1-0f64-4892-a5b3-65655e2f16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            wide_features = batch['wide'].to(device)\n",
    "            deep_features = batch['deep'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(wide_features, deep_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                wide_features = batch['wide'].to(device)\n",
    "                deep_features = batch['deep'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(wide_features, deep_features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                \n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_rmse = np.sqrt(np.mean((np.array(predictions) - np.array(true_labels)) ** 2))\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Validation RMSE: {val_rmse:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d74b0-83d0-4f81-b033-7629a3bdfd76",
   "metadata": {},
   "source": [
    "<font color='blue'> Setup Random Seed and Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b8a8a5a-e30b-44bf-86ab-a1e13b982f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "df = data_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30f00b-417a-4476-bd58-f2295d1e5c7d",
   "metadata": {},
   "source": [
    "<font color='blue'> Process the dataset and generate the wide and deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4eaf23-8a5c-4161-b44e-142ab1f21996",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df, wide_features, deep_features, encoders = process_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089218a-5219-4b0c-aa43-e201c7e4f085",
   "metadata": {},
   "source": [
    "<font color='blue'> Split the dataset into Trainm Test and Validation \\\n",
    "    <b> Train Size: 70% \\\n",
    "    Test Size: 15% \\\n",
    "    Val Size: 15%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe7545-ebfa-4834-ab78-547af00f5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, remaining_df = train_test_split(processed_df, test_size=0.3, random_state=42)\n",
    "\n",
    "val_df, test_df = train_test_split(remaining_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5e085-1ec3-4643-8200-e7973b934466",
   "metadata": {},
   "source": [
    "<font color='blue'> Setup the Datasets and Create Dataloaders with batch Size as 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ee29d-fdf2-4869-a388-a74074b274b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = AnimeDataset(train_df, wide_features, deep_features)\n",
    "val_dataset = AnimeDataset(val_df, wide_features, deep_features)\n",
    "test_dataset = AnimeDataset(test_df, wide_features, deep_features)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db799ef-4df3-4f34-aaa6-b59c305cb892",
   "metadata": {},
   "source": [
    "<font color='blue'> Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4beccb62-35c8-41b0-a3a9-03fd62044924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wide_dim = [len(encoders[f].classes_) for f in wide_features]\n",
    "deep_dim = len(deep_features)\n",
    "model = WideAndDeepModel(\n",
    "    wide_dim=wide_dim,\n",
    "    deep_dim=deep_dim,\n",
    "    wide_embeddings_dim=16,\n",
    "    deep_hidden_units=[128, 64, 32]\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1882a3-9219-4dd0-9ce4-597b89fd509c",
   "metadata": {},
   "source": [
    "<font color='blue'> Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dce7a2a-3d9f-4498-abf3-26621b28b6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 0.0882\n",
      "Validation Loss: 0.0075\n",
      "Validation RMSE: 0.0868\n",
      "Epoch 2/10\n",
      "Training Loss: 0.0156\n",
      "Validation Loss: 0.0074\n",
      "Validation RMSE: 0.0861\n",
      "Epoch 3/10\n",
      "Training Loss: 0.0152\n",
      "Validation Loss: 0.0060\n",
      "Validation RMSE: 0.0772\n",
      "Epoch 4/10\n",
      "Training Loss: 0.0149\n",
      "Validation Loss: 0.0075\n",
      "Validation RMSE: 0.0865\n",
      "Epoch 5/10\n",
      "Training Loss: 0.0148\n",
      "Validation Loss: 0.0062\n",
      "Validation RMSE: 0.0787\n",
      "Epoch 6/10\n",
      "Training Loss: 0.0147\n",
      "Validation Loss: 0.0057\n",
      "Validation RMSE: 0.0753\n",
      "Epoch 7/10\n",
      "Training Loss: 0.0146\n",
      "Validation Loss: 0.0052\n",
      "Validation RMSE: 0.0720\n",
      "Epoch 8/10\n",
      "Training Loss: 0.0146\n",
      "Validation Loss: 0.0052\n",
      "Validation RMSE: 0.0725\n",
      "Epoch 9/10\n",
      "Training Loss: 0.0144\n",
      "Validation Loss: 0.0050\n",
      "Validation RMSE: 0.0709\n",
      "Epoch 10/10\n",
      "Training Loss: 0.0142\n",
      "Validation Loss: 0.0053\n",
      "Validation RMSE: 0.0729\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd7571-5f2c-43e8-a39a-631e5cc5e03e",
   "metadata": {},
   "source": [
    "<font color='blue'> The training loss consistently decreases till the last epochs, indicating that the model is learning effectively from the training data. The RMSE values show a trend of decreasing until epoch 9, which indicates improving performance in terms of prediction accuracy on the validation set. There is minor increase with Epoch 10 for Validation RMSE, this could be a sign of Overfitting, however, without further epochs it cannot be established. \\\n",
    "Also, The model appears to be relatively stable, as the training loss values are relatively small and show a gradual decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa52954-9c8e-428b-a17e-1b305d0e6684",
   "metadata": {},
   "source": [
    "<font color='blue'> Generating Metrics for the Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e14a55c-0b2c-4faa-b72b-769d2e9d0d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_predictions(model, val_loader, val_df, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            wide_features = batch['wide'].to(device)\n",
    "            deep_features = batch['deep'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(wide_features, deep_features)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            \n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_rmse = np.sqrt(np.mean((np.array(predictions) - np.array(true_labels)) ** 2))\n",
    "    print(f'Validation Loss: {test_loss/len(test_loader):.4f}')\n",
    "    print(f'Test RMSE: {test_rmse:.4f}')\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    errors = np.abs(predictions - true_labels)\n",
    "    \n",
    "    val_df['predicted_score'] = predictions\n",
    "    val_df['prediction_error'] = errors\n",
    "    \n",
    "    print(\"\\nAnalysis by Genre:\")\n",
    "    genre_errors = val_df.groupby('Genres')['prediction_error'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "    print(genre_errors.head())\n",
    "    \n",
    "    print(\"\\nAnalysis by Type:\")\n",
    "    type_errors = val_df.groupby('Type')['prediction_error'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "    print(type_errors)\n",
    "    \n",
    "    print(\"\\nWorst Predictions:\")\n",
    "    worst_predictions = val_df.nlargest(5, 'prediction_error')[['Anime Title', 'Score', 'predicted_score', 'prediction_error']]\n",
    "    print(worst_predictions)\n",
    "    \n",
    "    return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10fe23da-3afe-4519-8cdc-417f40bc8423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0053\n",
      "Test RMSE: 0.0729\n",
      "\n",
      "Analysis by Genre:\n",
      "            mean  count\n",
      "Genres                 \n",
      "532     1.361262      1\n",
      "538     0.905021     81\n",
      "515     0.817674     49\n",
      "907     0.686859    574\n",
      "506     0.582172    414\n",
      "\n",
      "Analysis by Type:\n",
      "          mean    count\n",
      "Type                   \n",
      "1     0.344736     6180\n",
      "3     0.071914   170510\n",
      "2     0.069820    29616\n",
      "4     0.038799   108937\n",
      "0     0.037054   245114\n",
      "5     0.029681  1224887\n",
      "\n",
      "Worst Predictions:\n",
      "           Anime Title  Score  predicted_score  prediction_error\n",
      "23646117  Tsui no Sora   2.22         6.118314          3.898314\n",
      "23646180  Tsui no Sora   2.22         6.117153          3.897153\n",
      "23646122  Tsui no Sora   2.22         6.113920          3.893920\n",
      "23646173  Tsui no Sora   2.22         6.109069          3.889069\n",
      "23646171  Tsui no Sora   2.22         6.105966          3.885966\n"
     ]
    }
   ],
   "source": [
    "val_df_with_predictions = analyze_predictions(model, val_loader, val_df, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef015f9-998e-4a4d-86e3-b7a925b85aae",
   "metadata": {},
   "source": [
    "<font color='blue'> Evaluation \\\n",
    "<b>Validation Loss:</b>Validation loss of 0.0053 is low which indicating that the model has trained effectively and is able to generalize well on the validation data. \\\n",
    "<b>Test RMSE:</b> Test RMSE of 0.0729 suggests that the predictions are close to the actual values on the val dataset. Given that this RMSE is reasonably low, it indicates that the model's predictions are generally accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2836052-0095-4462-9fce-e4d19e2694a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 22457891     111125\n",
       "4987336     1158121\n",
       "15293953     919863\n",
       "14926919     455244\n",
       "9502097      496828\n",
       "             ...   \n",
       "8153754      422506\n",
       "22579532     366396\n",
       "22380835     331549\n",
       "18118120     396311\n",
       "5043314      381646\n",
       "Name: user_id, Length: 1785244, dtype: int64>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['user_id'].unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b099b-52ca-4c69-a7fc-6971ecdb3e0b",
   "metadata": {},
   "source": [
    "<font color='blue'> Display Dataframe for one User ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85f593d6-2347-4764-9331-27319c79d18b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>Username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>Anime Title</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Mean Score</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16317179</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>49438</td>\n",
       "      <td>Isekai Yakkyoku</td>\n",
       "      <td>850</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16355083</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>48761</td>\n",
       "      <td>Saihate no Paladin</td>\n",
       "      <td>80</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14539718</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>23283</td>\n",
       "      <td>Zankyou no Terror</td>\n",
       "      <td>929</td>\n",
       "      <td>7.71</td>\n",
       "      <td>8.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9071784</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>7088</td>\n",
       "      <td>Ichiban Ushiro no Daimaou</td>\n",
       "      <td>176</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19667017</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>49891</td>\n",
       "      <td>Tensei shitara Ken deshita</td>\n",
       "      <td>270</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006093</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>8937</td>\n",
       "      <td>Toaru Majutsu no Index II</td>\n",
       "      <td>290</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951342</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>523</td>\n",
       "      <td>Tonari no Totoro</td>\n",
       "      <td>371</td>\n",
       "      <td>7.71</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14327608</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>21561</td>\n",
       "      <td>Ryuugajou Nanana no Maizoukin</td>\n",
       "      <td>413</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969642</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>16049</td>\n",
       "      <td>Toaru Kagaku no Railgun S</td>\n",
       "      <td>290</td>\n",
       "      <td>7.71</td>\n",
       "      <td>8.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18469208</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>32105</td>\n",
       "      <td>Sousei no Onmyouji</td>\n",
       "      <td>286</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23097263</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>7875</td>\n",
       "      <td>Kanokon: Manatsu no Dai Shanikusai Specials</td>\n",
       "      <td>848</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23089981</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>4037</td>\n",
       "      <td>Cowboy Bebop: Yose Atsume Blues</td>\n",
       "      <td>941</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17227494</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>28623</td>\n",
       "      <td>Koutetsujou no Kabaneri</td>\n",
       "      <td>227</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6335335</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>15583</td>\n",
       "      <td>Date A Live</td>\n",
       "      <td>934</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992369</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>6</td>\n",
       "      <td>Trigun</td>\n",
       "      <td>111</td>\n",
       "      <td>7.71</td>\n",
       "      <td>8.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23646901</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>21235</td>\n",
       "      <td>Imanara Maniau! Log Horizon</td>\n",
       "      <td>80</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12815899</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>50</td>\n",
       "      <td>Aa! Megami-sama! (TV)</td>\n",
       "      <td>751</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14684256</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>11887</td>\n",
       "      <td>Kokoro Connect</td>\n",
       "      <td>832</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21146523</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>9062</td>\n",
       "      <td>Angel Beats! Specials</td>\n",
       "      <td>767</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19037723</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>21511</td>\n",
       "      <td>Kantai Collection: KanColle</td>\n",
       "      <td>338</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17203553</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>15379</td>\n",
       "      <td>Kotoura-san</td>\n",
       "      <td>666</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18536605</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>50854</td>\n",
       "      <td>Benriya Saitou-san, Isekai ni Iku</td>\n",
       "      <td>391</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22547048</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>8440</td>\n",
       "      <td>Black Lagoon Omake</td>\n",
       "      <td>646</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22579532</th>\n",
       "      <td>366396</td>\n",
       "      <td>shinji144</td>\n",
       "      <td>22433</td>\n",
       "      <td>Break Blade</td>\n",
       "      <td>270</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id   Username  anime_id  \\\n",
       "16317179   366396  shinji144     49438   \n",
       "16355083   366396  shinji144     48761   \n",
       "14539718   366396  shinji144     23283   \n",
       "9071784    366396  shinji144      7088   \n",
       "19667017   366396  shinji144     49891   \n",
       "10006093   366396  shinji144      8937   \n",
       "3951342    366396  shinji144       523   \n",
       "14327608   366396  shinji144     21561   \n",
       "9969642    366396  shinji144     16049   \n",
       "18469208   366396  shinji144     32105   \n",
       "23097263   366396  shinji144      7875   \n",
       "23089981   366396  shinji144      4037   \n",
       "17227494   366396  shinji144     28623   \n",
       "6335335    366396  shinji144     15583   \n",
       "3992369    366396  shinji144         6   \n",
       "23646901   366396  shinji144     21235   \n",
       "12815899   366396  shinji144        50   \n",
       "14684256   366396  shinji144     11887   \n",
       "21146523   366396  shinji144      9062   \n",
       "19037723   366396  shinji144     21511   \n",
       "17203553   366396  shinji144     15379   \n",
       "18536605   366396  shinji144     50854   \n",
       "22547048   366396  shinji144      8440   \n",
       "22579532   366396  shinji144     22433   \n",
       "\n",
       "                                          Anime Title  Genres  Mean Score  \\\n",
       "16317179                              Isekai Yakkyoku     850        7.71   \n",
       "16355083                           Saihate no Paladin      80        7.71   \n",
       "14539718                            Zankyou no Terror     929        7.71   \n",
       "9071784                     Ichiban Ushiro no Daimaou     176        7.71   \n",
       "19667017                   Tensei shitara Ken deshita     270        7.71   \n",
       "10006093                    Toaru Majutsu no Index II     290        7.71   \n",
       "3951342                              Tonari no Totoro     371        7.71   \n",
       "14327608                Ryuugajou Nanana no Maizoukin     413        7.71   \n",
       "9969642                     Toaru Kagaku no Railgun S     290        7.71   \n",
       "18469208                           Sousei no Onmyouji     286        7.71   \n",
       "23097263  Kanokon: Manatsu no Dai Shanikusai Specials     848        7.71   \n",
       "23089981              Cowboy Bebop: Yose Atsume Blues     941        7.71   \n",
       "17227494                      Koutetsujou no Kabaneri     227        7.71   \n",
       "6335335                                   Date A Live     934        7.71   \n",
       "3992369                                        Trigun     111        7.71   \n",
       "23646901                  Imanara Maniau! Log Horizon      80        7.71   \n",
       "12815899                        Aa! Megami-sama! (TV)     751        7.71   \n",
       "14684256                               Kokoro Connect     832        7.71   \n",
       "21146523                        Angel Beats! Specials     767        7.71   \n",
       "19037723                  Kantai Collection: KanColle     338        7.71   \n",
       "17203553                                  Kotoura-san     666        7.71   \n",
       "18536605            Benriya Saitou-san, Isekai ni Iku     391        7.71   \n",
       "22547048                           Black Lagoon Omake     646        7.71   \n",
       "22579532                                  Break Blade     270        7.71   \n",
       "\n",
       "          Score  \n",
       "16317179   7.32  \n",
       "16355083   6.84  \n",
       "14539718   8.10  \n",
       "9071784    6.74  \n",
       "19667017   7.55  \n",
       "10006093   7.53  \n",
       "3951342    8.25  \n",
       "14327608   7.13  \n",
       "9969642    8.02  \n",
       "18469208   7.30  \n",
       "23097263   6.48  \n",
       "23089981   7.41  \n",
       "17227494   7.27  \n",
       "6335335    7.16  \n",
       "3992369    8.22  \n",
       "23646901   6.76  \n",
       "12815899   7.32  \n",
       "14684256   7.75  \n",
       "21146523   7.55  \n",
       "19037723   6.83  \n",
       "17203553   7.17  \n",
       "18536605   7.48  \n",
       "22547048   6.77  \n",
       "22579532   7.25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_df.query(f\"user_id == {366396}\")[['user_id', 'Username','anime_id', 'Anime Title', 'Genres', 'Mean Score', 'Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fc848-8602-4824-934a-8c376a873905",
   "metadata": {},
   "source": [
    "<font color='blue'> Generate Recommendations for a Single User to see how the model recommends Animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0ca3cdb-0081-4108-9092-5851383ccb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/07/qch9szx51kdgzhzp77njqrqw001f2_/T/ipykernel_64075/2499454855.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unrated_animes['predicted_score'] = predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>Anime Title</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Mean Score</th>\n",
       "      <th>Score</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>652279</th>\n",
       "      <td>1</td>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td>143</td>\n",
       "      <td>8.30</td>\n",
       "      <td>8.75000</td>\n",
       "      <td>9.243464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138439</th>\n",
       "      <td>9253</td>\n",
       "      <td>Steins;Gate</td>\n",
       "      <td>838</td>\n",
       "      <td>7.50</td>\n",
       "      <td>9.07000</td>\n",
       "      <td>9.221704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23800621</th>\n",
       "      <td>51836</td>\n",
       "      <td>Douluo Dalu II: Jueshi Tangmen</td>\n",
       "      <td>80</td>\n",
       "      <td>7.79</td>\n",
       "      <td>6.38089</td>\n",
       "      <td>9.220238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17270587</th>\n",
       "      <td>820</td>\n",
       "      <td>Ginga Eiyuu Densetsu</td>\n",
       "      <td>833</td>\n",
       "      <td>8.41</td>\n",
       "      <td>9.02000</td>\n",
       "      <td>9.158541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17270730</th>\n",
       "      <td>820</td>\n",
       "      <td>Ginga Eiyuu Densetsu</td>\n",
       "      <td>833</td>\n",
       "      <td>7.91</td>\n",
       "      <td>9.02000</td>\n",
       "      <td>9.153250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anime_id                     Anime Title  Genres  Mean Score  \\\n",
       "652279           1                    Cowboy Bebop     143        8.30   \n",
       "8138439       9253                     Steins;Gate     838        7.50   \n",
       "23800621     51836  Douluo Dalu II: Jueshi Tangmen      80        7.79   \n",
       "17270587       820            Ginga Eiyuu Densetsu     833        8.41   \n",
       "17270730       820            Ginga Eiyuu Densetsu     833        7.91   \n",
       "\n",
       "            Score  predicted_score  \n",
       "652279    8.75000         9.243464  \n",
       "8138439   9.07000         9.221704  \n",
       "23800621  6.38089         9.220238  \n",
       "17270587  9.02000         9.158541  \n",
       "17270730  9.02000         9.153250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_recommendations(user_id, top_n=5, test_df=test_df, wide_features=wide_features, deep_features=deep_features, model=model):\n",
    "    user_data = test_df[test_df['user_id'] == user_id]\n",
    "    if user_data.empty:\n",
    "        return \"User not found\"\n",
    "    \n",
    "    # Get all animes not rated by the user\n",
    "    unrated_animes = test_df[~test_df['anime_id'].isin(user_data['anime_id'])]\n",
    "    \n",
    "    # Create a dataset for unrated animes\n",
    "    unrated_dataset = AnimeDataset(unrated_animes, wide_features, deep_features)\n",
    "    unrated_loader = DataLoader(unrated_dataset, batch_size=256)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in unrated_loader:\n",
    "            wide_input = batch['wide']\n",
    "            deep_input = batch['deep']\n",
    "            outputs = model(wide_input, deep_input)\n",
    "            predictions.extend(outputs.squeeze().tolist())\n",
    "    \n",
    "    # Get top N recommendations\n",
    "    unrated_animes['predicted_score'] = predictions\n",
    "    top_recommendations = unrated_animes.nlargest(top_n, 'predicted_score')\n",
    "    \n",
    "    return top_recommendations[['anime_id', 'Anime Title', 'Genres', 'Mean Score', 'Score', 'predicted_score']]\n",
    "\n",
    "# Example usage\n",
    "recommendations = get_recommendations(user_id=366396, top_n=5)\n",
    "display(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69f6a9-26ca-4a5c-9d8e-16fd331fd354",
   "metadata": {},
   "source": [
    "<font color='blue'> Based on the Original ratings given by the user. Our model recommended new Animes that the user can review and watch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25497e1c-edd4-4223-bb4e-6ff315bd5749",
   "metadata": {},
   "source": [
    "## <font color='blue'> Thank You"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
